{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Squared L2 Norm Function:\n",
    "\n",
    "$\\sqrt{\\Sigma w_i^2} ^ 2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0, loss: 5793.811507862259, accuracy: 0.8196\n",
      "iter: 1, loss: 5531.492067116258, accuracy: 0.8196\n",
      "iter: 2, loss: 5217.3737400898735, accuracy: 0.8194\n",
      "iter: 3, loss: 4603.570963825092, accuracy: 0.8212\n",
      "iter: 4, loss: 4229.555574459421, accuracy: 0.8192\n",
      "iter: 5, loss: 3953.6292325296454, accuracy: 0.8212\n",
      "iter: 6, loss: 3959.666941502819, accuracy: 0.823\n",
      "iter: 7, loss: 3713.612427443597, accuracy: 0.8218\n",
      "iter: 8, loss: 3172.2861609198026, accuracy: 0.814\n",
      "iter: 9, loss: 3034.5649577972554, accuracy: 0.8072\n",
      "iter: 10, loss: 2831.6970514607424, accuracy: 0.8054\n",
      "iter: 11, loss: 2628.1126175321942, accuracy: 0.8064\n",
      "iter: 12, loss: 2504.702827297553, accuracy: 0.8114\n",
      "iter: 13, loss: 2435.3492857116044, accuracy: 0.813\n",
      "iter: 14, loss: 2388.010077070792, accuracy: 0.8124\n",
      "iter: 15, loss: 2283.8172159756323, accuracy: 0.8166\n",
      "iter: 16, loss: 2239.3687537582864, accuracy: 0.8178\n",
      "iter: 17, loss: 2238.32187272549, accuracy: 0.8272\n",
      "iter: 18, loss: 2131.870205159041, accuracy: 0.831\n",
      "iter: 19, loss: 2126.0638275307765, accuracy: 0.8292\n",
      "iter: 20, loss: 2090.450743820381, accuracy: 0.835\n",
      "iter: 21, loss: 2047.8931753410748, accuracy: 0.8352\n",
      "iter: 22, loss: 1942.4617586825734, accuracy: 0.8394\n",
      "iter: 23, loss: 1961.321164063074, accuracy: 0.8364\n",
      "iter: 24, loss: 1903.176878405635, accuracy: 0.8394\n",
      "iter: 25, loss: 1927.4387941677942, accuracy: 0.8364\n",
      "iter: 26, loss: 1893.5026592129796, accuracy: 0.8392\n",
      "iter: 27, loss: 2021.3108073020192, accuracy: 0.833\n",
      "iter: 28, loss: 1980.9787727020023, accuracy: 0.8324\n",
      "iter: 29, loss: 1898.2227325162746, accuracy: 0.8344\n",
      "iter: 30, loss: 1754.303477034027, accuracy: 0.8444\n",
      "iter: 31, loss: 1728.470861777049, accuracy: 0.8466\n",
      "iter: 32, loss: 1696.2678825287812, accuracy: 0.8522\n",
      "iter: 33, loss: 1698.8318037471347, accuracy: 0.85\n",
      "iter: 34, loss: 1701.1733587484164, accuracy: 0.8524\n",
      "iter: 35, loss: 1656.771122832542, accuracy: 0.853\n",
      "iter: 36, loss: 1636.9332599559648, accuracy: 0.8542\n",
      "iter: 37, loss: 1626.5410998330633, accuracy: 0.8494\n",
      "iter: 38, loss: 1607.0511822821059, accuracy: 0.85\n",
      "iter: 39, loss: 1616.7624499367132, accuracy: 0.8432\n",
      "iter: 40, loss: 1624.2170804642765, accuracy: 0.838\n",
      "iter: 41, loss: 1574.9185871460309, accuracy: 0.8584\n",
      "iter: 42, loss: 1551.1311245311924, accuracy: 0.846\n",
      "iter: 43, loss: 1535.8673310939591, accuracy: 0.8576\n",
      "iter: 44, loss: 1522.5218164342434, accuracy: 0.8438\n",
      "iter: 45, loss: 1503.3046839214242, accuracy: 0.8442\n",
      "iter: 46, loss: 1495.0380612372282, accuracy: 0.8436\n",
      "iter: 47, loss: 1497.3685907424895, accuracy: 0.8404\n",
      "iter: 48, loss: 1493.0764976649245, accuracy: 0.8422\n",
      "iter: 49, loss: 1495.8865376977662, accuracy: 0.851\n",
      "iter: 50, loss: 1484.866676963168, accuracy: 0.8444\n",
      "iter: 51, loss: 1497.7053056315997, accuracy: 0.8378\n",
      "iter: 52, loss: 1539.5635094459687, accuracy: 0.8312\n",
      "iter: 53, loss: 1513.9317764664333, accuracy: 0.8342\n",
      "iter: 54, loss: 1515.9060016992091, accuracy: 0.8372\n",
      "iter: 55, loss: 1515.2727690229651, accuracy: 0.84\n",
      "iter: 56, loss: 1531.9242011940976, accuracy: 0.8516\n",
      "iter: 57, loss: 1632.140193966229, accuracy: 0.8684\n",
      "iter: 58, loss: 1521.0692694945717, accuracy: 0.8484\n",
      "iter: 59, loss: 1581.1078825372188, accuracy: 0.865\n",
      "iter: 60, loss: 1620.0655429665821, accuracy: 0.8674\n",
      "iter: 61, loss: 1580.0193188899084, accuracy: 0.8638\n",
      "iter: 62, loss: 1512.0691603332612, accuracy: 0.852\n",
      "iter: 63, loss: 1490.8659212676246, accuracy: 0.842\n",
      "iter: 64, loss: 1501.1757758045833, accuracy: 0.8336\n",
      "iter: 65, loss: 1687.07961797316, accuracy: 0.8784\n",
      "iter: 66, loss: 1696.0952667256533, accuracy: 0.8798\n",
      "iter: 67, loss: 1528.0316520607719, accuracy: 0.8554\n",
      "iter: 68, loss: 1517.5775696144676, accuracy: 0.8502\n",
      "iter: 69, loss: 1521.9430030832557, accuracy: 0.8502\n",
      "iter: 70, loss: 1536.6484153156869, accuracy: 0.8514\n",
      "iter: 71, loss: 1536.8305408551907, accuracy: 0.8446\n",
      "iter: 72, loss: 1608.45364248483, accuracy: 0.8624\n",
      "iter: 73, loss: 1511.4546909218618, accuracy: 0.8388\n",
      "iter: 74, loss: 1509.350037695985, accuracy: 0.8454\n",
      "iter: 75, loss: 1514.7978393660176, accuracy: 0.847\n",
      "iter: 76, loss: 1613.2916229976404, accuracy: 0.8722\n",
      "iter: 77, loss: 1499.087494212822, accuracy: 0.8462\n",
      "iter: 78, loss: 1567.7573893061403, accuracy: 0.8642\n",
      "iter: 79, loss: 1502.8125214016477, accuracy: 0.8474\n",
      "iter: 80, loss: 1569.9130464645116, accuracy: 0.8648\n",
      "iter: 81, loss: 1601.8103603393915, accuracy: 0.8688\n",
      "iter: 82, loss: 1495.606249436781, accuracy: 0.8442\n",
      "iter: 83, loss: 1874.654041786439, accuracy: 0.8806\n",
      "iter: 84, loss: 1591.7372581818643, accuracy: 0.8674\n",
      "iter: 85, loss: 1491.2318054327197, accuracy: 0.8404\n",
      "iter: 86, loss: 1485.1090486178925, accuracy: 0.8406\n",
      "iter: 87, loss: 1486.163401578486, accuracy: 0.8444\n",
      "iter: 88, loss: 1504.6973357460174, accuracy: 0.8374\n",
      "iter: 89, loss: 1516.4967292794563, accuracy: 0.8342\n",
      "iter: 90, loss: 1508.7081854772964, accuracy: 0.8342\n",
      "iter: 91, loss: 1510.5974055678141, accuracy: 0.8472\n",
      "iter: 92, loss: 1578.5386245736588, accuracy: 0.8636\n",
      "iter: 93, loss: 1501.025035748067, accuracy: 0.846\n",
      "iter: 94, loss: 1524.4788617631873, accuracy: 0.8484\n",
      "iter: 95, loss: 1530.8192223358428, accuracy: 0.8474\n",
      "iter: 96, loss: 1530.7582483235385, accuracy: 0.8474\n",
      "iter: 97, loss: 1528.539207461264, accuracy: 0.8496\n",
      "iter: 98, loss: 1506.157876546288, accuracy: 0.8376\n",
      "iter: 99, loss: 1504.536506418139, accuracy: 0.838\n",
      "iter: 100, loss: 1504.002942761985, accuracy: 0.8382\n"
     ]
    }
   ],
   "source": [
    "def logistic(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def log_prob(z, y_i):\n",
    "    '''\n",
    "    Returns the log_prob for one point\n",
    "    '''\n",
    "    fz = logistic(z)\n",
    "    return y_i * np.log(fz) + (1 - y_i) * np.log(1 - fz)\n",
    "\n",
    "\n",
    "def neg_log_likelihood(X, w, y):\n",
    "    '''Compute the negative log likelihood'''\n",
    "    L = 0\n",
    "    for _x,_y in zip(X, y):\n",
    "        z = w.dot(_x)\n",
    "        L += log_prob(z=z, y_i=_y)\n",
    "    return -1 * L\n",
    "\n",
    "\n",
    "def fast_logistic(X, w):\n",
    "    '''Compute the logistic function over many data points'''\n",
    "    return 1/(1 + np.exp(-1 * X.dot(w)))\n",
    "\n",
    "\n",
    "def grad(_X, w, _y, lambda_=.5):\n",
    "    '''\n",
    "    Return the gradient\n",
    "    \n",
    "    - https://web.stanford.edu/~jurafsky/slp3/5.pdf\n",
    "    '''\n",
    "    grad = np.zeros_like(w)\n",
    "    \n",
    "    N,D= _X.shape\n",
    "    \n",
    "    b = _X * (fast_logistic(_X, w) - _y).reshape((N, 1))\n",
    "\n",
    "    return np.sum(b, axis=0) + (lambda_ * 2 * w)\n",
    "\n",
    "\n",
    "def squared_l2_norm(w):\n",
    "    '''\n",
    "    Return the L2 norm of the weights, squared. \n",
    "    \n",
    "    Remember that we square the norm of the weights,\n",
    "    to make the math easier when computing the gradients\n",
    "    \n",
    "    $\\sqrt{\\Sigma w_i^2} ^ 2\n",
    "    '''\n",
    "    norm = 0\n",
    "    for weight in w:\n",
    "        norm += weight** 2\n",
    "    l2 = math.sqrt(norm)\n",
    "    squared_l2 = l2 ** 2\n",
    "    return squared_l2\n",
    "\n",
    "#tolerance is any small number close to 0 that you specify (hyperparam)\n",
    "def grad_descent(_X, _y, eta = .0001, lambda_ = 0, tolerance=1e-4, verbose=True, batch_size=None, iters=None):\n",
    "    '''\n",
    "    Perform gradient descent\n",
    "    '''\n",
    "    w = np.random.uniform(low=-5, high=2, size=dim_)\n",
    "    \n",
    "    losses = []\n",
    "    for i in range(1000):\n",
    "        if i > iters and iters is not None:\n",
    "            break\n",
    "        #this basically is the L(w) + lambda*squared_L2_norm\n",
    "        #This is our regularization that penalizes big weights\n",
    "        this_ll = neg_log_likelihood(_X, w, _y)\n",
    "        loss = this_ll + lambda_ * squared_l2_norm(w)\n",
    "        losses.append(loss)\n",
    "        if verbose:\n",
    "            print(\"iter: {}, loss: {}, accuracy: {}\".format(i, loss, accuracy(_X, w, _y)))\n",
    "        \n",
    "        #the tolerance is our threshold as we approach 0\n",
    "        #this code is checking how far away you are from 0\n",
    "        #if you are close enough, we will call it a day (to not waste time getting to exactly 0)\n",
    "        if (squared_l2_norm(grad(_X, w, _y, lambda_=lambda_))) < tolerance:\n",
    "            break\n",
    "        \n",
    "        #This is to help us be more efficient computing the gradient\n",
    "        #Stochastic gradient descent: getting gradients randomly to get to the minimum\n",
    "        #Batch_size --> if it's none, we will compute the whole gradient\n",
    "        #Otherwise, we will select random instances and will compute the gradient based on\n",
    "        #that chosen batch\n",
    "        #small batch size will make our loss more volatile, but generally go down\n",
    "        #if the batch size is really big, it will really smoothly decrease\n",
    "        #(bc its a big sample of the gradient), so less noisy\n",
    "        #but it will take a lot longer to compute!\n",
    "        if batch_size is None:\n",
    "            w -= eta * grad(_X, w, _y, lambda_=lambda_)\n",
    "        else:\n",
    "            _N,F = _X.shape\n",
    "            idx = np.random.randint(_N, size=batch_size)\n",
    "            w -= eta * grad(_X[idx], w, _y[idx], lambda_=lambda_)/batch_size\n",
    "        \n",
    "    return w, losses\n",
    "\n",
    "def prediction(X, w, threshold=.5):\n",
    "    '''\n",
    "    - Return a Boolean array of length N.\n",
    "    - The array should be True if the weights dotted with the features for a given instance is greater than .5\n",
    "    '''\n",
    "    N, D = X.shape\n",
    "    return X.dot(w) > threshold\n",
    "\n",
    "def accuracy(X, w, y):\n",
    "    '''\n",
    "    Return a value between 0 and 1, showing the fraction of data points which have been classified correctly\n",
    "    '''\n",
    "    return np.mean(prediction(X, w) == y)\n",
    "\n",
    "def init_data(N, dim_):\n",
    "    '''\n",
    "    Initialize data. Note how we generate y below. We know how the data is generated.\n",
    "    '''\n",
    "    #Generating random data, and random labels according to some random process\n",
    "    w = np.random.uniform(low=-1, high=1, size=dim_)\n",
    "    X = (np.random.rand(dim_ * N) > .5).astype(int)\n",
    "    X = X.reshape(N, dim_)\n",
    "\n",
    "    z_ = X.dot(w) + np.random.uniform(low=-1, high=1, size=X.dot(w).size)\n",
    "\n",
    "    y =  1/(1 + np.exp(-1 * z_)) > .5\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "N = 10000 #Number of instances in our dataset\n",
    "dim_ = 10 #Number of dimensions (features)\n",
    "\n",
    "#Instantiating the weight vector as a uniform distribution\n",
    "#low sample is -5, high sample is 2, and the number of items is our dim #\n",
    "#all random numbers\n",
    "w = np.random.uniform(low=-5, high=2, size=dim_)\n",
    "\n",
    "#X is N points with dim_ features\n",
    "#y is N points with 1 label\n",
    "X, y = init_data(N, dim_)\n",
    "\n",
    "# splitting the data in half\n",
    "split = int(N/2)\n",
    "\n",
    "X_train = X[0:split]\n",
    "X_test = X[split:]\n",
    "y_train = y[0:split]\n",
    "y_test = y[split:]\n",
    "\n",
    "#regularization penalty, we multiply the norm of the gradient\n",
    "#by this number to penalize larger weights\n",
    "lambda_ = .1\n",
    "\n",
    "w, losses = grad_descent(X_train, y_train,\n",
    "                        eta=1, tolerance=.0001,\n",
    "                        iters=100, verbose=True,\n",
    "                        lambda_=lambda_, batch_size=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make batch size really small (1 instead of 10)\n",
    "#verbose is False (why?) oh, it doesn't print out the solution I see\n",
    "w, losses_small = grad_descent(X_train, y_train,\n",
    "                        eta=1, tolerance=.0001,\n",
    "                        iters=100, verbose=False,\n",
    "                        lambda_=lambda_, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iter</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8995.631731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7219.308030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3317.651849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3736.098317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3181.592848</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   iter batch_size         loss\n",
       "0     0          1  8995.631731\n",
       "1     1          1  7219.308030\n",
       "2     2          1  3317.651849\n",
       "3     3          1  3736.098317\n",
       "4     4          1  3181.592848"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#making a dataframe for the iteration and loss at that iteration\n",
    "index = [i for i in range(len(losses))]\n",
    "kind = [\"1\" for i in range(len(losses))]\n",
    "kind = kind + [\"10\" for i in range(len(losses))]\n",
    "df = pd.DataFrame({\"iter\": index + index,\n",
    "                   \"batch_size\":kind,\n",
    "                   \"loss\":losses_small + losses})\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-107ef3e2abc4425a92dc864e6a8752d5\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-107ef3e2abc4425a92dc864e6a8752d5\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-107ef3e2abc4425a92dc864e6a8752d5\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-c3beb5f829deef5aba55985a6a95e236\"}, \"mark\": \"line\", \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"batch_size\"}, \"x\": {\"type\": \"quantitative\", \"field\": \"iter\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"loss\"}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-c3beb5f829deef5aba55985a6a95e236\": [{\"iter\": 0, \"batch_size\": \"1\", \"loss\": 8995.631731163527}, {\"iter\": 1, \"batch_size\": \"1\", \"loss\": 7219.3080296564785}, {\"iter\": 2, \"batch_size\": \"1\", \"loss\": 3317.651849069687}, {\"iter\": 3, \"batch_size\": \"1\", \"loss\": 3736.098316652653}, {\"iter\": 4, \"batch_size\": \"1\", \"loss\": 3181.5928475589417}, {\"iter\": 5, \"batch_size\": \"1\", \"loss\": 2756.928656868239}, {\"iter\": 6, \"batch_size\": \"1\", \"loss\": 2990.796891007469}, {\"iter\": 7, \"batch_size\": \"1\", \"loss\": 2227.268974010346}, {\"iter\": 8, \"batch_size\": \"1\", \"loss\": 2337.1774301372448}, {\"iter\": 9, \"batch_size\": \"1\", \"loss\": 2251.5790228974024}, {\"iter\": 10, \"batch_size\": \"1\", \"loss\": 2247.421989711018}, {\"iter\": 11, \"batch_size\": \"1\", \"loss\": 5394.669402977515}, {\"iter\": 12, \"batch_size\": \"1\", \"loss\": 2373.582975520008}, {\"iter\": 13, \"batch_size\": \"1\", \"loss\": 2316.840277040879}, {\"iter\": 14, \"batch_size\": \"1\", \"loss\": 2266.8872532125056}, {\"iter\": 15, \"batch_size\": \"1\", \"loss\": 2211.8948204579187}, {\"iter\": 16, \"batch_size\": \"1\", \"loss\": 2216.4610138427374}, {\"iter\": 17, \"batch_size\": \"1\", \"loss\": 3870.4351083937454}, {\"iter\": 18, \"batch_size\": \"1\", \"loss\": 2510.031797087548}, {\"iter\": 19, \"batch_size\": \"1\", \"loss\": 1998.714171921994}, {\"iter\": 20, \"batch_size\": \"1\", \"loss\": 2056.793973410043}, {\"iter\": 21, \"batch_size\": \"1\", \"loss\": 2073.3605075583464}, {\"iter\": 22, \"batch_size\": \"1\", \"loss\": 2123.881079790145}, {\"iter\": 23, \"batch_size\": \"1\", \"loss\": 2154.5180362462406}, {\"iter\": 24, \"batch_size\": \"1\", \"loss\": 2215.4866240900633}, {\"iter\": 25, \"batch_size\": \"1\", \"loss\": 2187.590351970765}, {\"iter\": 26, \"batch_size\": \"1\", \"loss\": 2212.2572389224483}, {\"iter\": 27, \"batch_size\": \"1\", \"loss\": 2290.9248117295365}, {\"iter\": 28, \"batch_size\": \"1\", \"loss\": 2217.2162826996996}, {\"iter\": 29, \"batch_size\": \"1\", \"loss\": 2178.934451874272}, {\"iter\": 30, \"batch_size\": \"1\", \"loss\": 2216.8622902310594}, {\"iter\": 31, \"batch_size\": \"1\", \"loss\": 2209.6097960685247}, {\"iter\": 32, \"batch_size\": \"1\", \"loss\": 2245.9958359426114}, {\"iter\": 33, \"batch_size\": \"1\", \"loss\": 2224.3356690881924}, {\"iter\": 34, \"batch_size\": \"1\", \"loss\": 2149.9705922015864}, {\"iter\": 35, \"batch_size\": \"1\", \"loss\": 2136.2773977677725}, {\"iter\": 36, \"batch_size\": \"1\", \"loss\": 2056.100574958641}, {\"iter\": 37, \"batch_size\": \"1\", \"loss\": 2073.9682187020794}, {\"iter\": 38, \"batch_size\": \"1\", \"loss\": 2079.05784605516}, {\"iter\": 39, \"batch_size\": \"1\", \"loss\": 2052.5962733348424}, {\"iter\": 40, \"batch_size\": \"1\", \"loss\": 2055.6485501691086}, {\"iter\": 41, \"batch_size\": \"1\", \"loss\": 2094.679608391734}, {\"iter\": 42, \"batch_size\": \"1\", \"loss\": 2075.3837162776235}, {\"iter\": 43, \"batch_size\": \"1\", \"loss\": 2082.251110769065}, {\"iter\": 44, \"batch_size\": \"1\", \"loss\": 2124.2584854796582}, {\"iter\": 45, \"batch_size\": \"1\", \"loss\": 2136.7495344213326}, {\"iter\": 46, \"batch_size\": \"1\", \"loss\": 2116.8820801182706}, {\"iter\": 47, \"batch_size\": \"1\", \"loss\": 2143.2725270620417}, {\"iter\": 48, \"batch_size\": \"1\", \"loss\": 2256.2450021104373}, {\"iter\": 49, \"batch_size\": \"1\", \"loss\": 2297.72807493888}, {\"iter\": 50, \"batch_size\": \"1\", \"loss\": 2265.798175860101}, {\"iter\": 51, \"batch_size\": \"1\", \"loss\": 2231.1815393749907}, {\"iter\": 52, \"batch_size\": \"1\", \"loss\": 2221.757292500915}, {\"iter\": 53, \"batch_size\": \"1\", \"loss\": 2201.1917422671468}, {\"iter\": 54, \"batch_size\": \"1\", \"loss\": 2216.4269864446946}, {\"iter\": 55, \"batch_size\": \"1\", \"loss\": 2221.4507714199494}, {\"iter\": 56, \"batch_size\": \"1\", \"loss\": 2194.593112831976}, {\"iter\": 57, \"batch_size\": \"1\", \"loss\": 4967.894253537372}, {\"iter\": 58, \"batch_size\": \"1\", \"loss\": 2378.3196614840895}, {\"iter\": 59, \"batch_size\": \"1\", \"loss\": 2294.743821857172}, {\"iter\": 60, \"batch_size\": \"1\", \"loss\": 2171.799197838063}, {\"iter\": 61, \"batch_size\": \"1\", \"loss\": 2115.309254473014}, {\"iter\": 62, \"batch_size\": \"1\", \"loss\": 3697.0939923364162}, {\"iter\": 63, \"batch_size\": \"1\", \"loss\": 2106.3250240562306}, {\"iter\": 64, \"batch_size\": \"1\", \"loss\": 2108.191931662128}, {\"iter\": 65, \"batch_size\": \"1\", \"loss\": 6280.2269184779925}, {\"iter\": 66, \"batch_size\": \"1\", \"loss\": 2197.6444814456113}, {\"iter\": 67, \"batch_size\": \"1\", \"loss\": 2109.67250515383}, {\"iter\": 68, \"batch_size\": \"1\", \"loss\": 2159.988980040457}, {\"iter\": 69, \"batch_size\": \"1\", \"loss\": 3522.667079642338}, {\"iter\": 70, \"batch_size\": \"1\", \"loss\": 2028.5405975252154}, {\"iter\": 71, \"batch_size\": \"1\", \"loss\": 2011.1294916806362}, {\"iter\": 72, \"batch_size\": \"1\", \"loss\": 2036.223447211215}, {\"iter\": 73, \"batch_size\": \"1\", \"loss\": 2025.344606059793}, {\"iter\": 74, \"batch_size\": \"1\", \"loss\": 2008.9154260258633}, {\"iter\": 75, \"batch_size\": \"1\", \"loss\": 2082.4609243804816}, {\"iter\": 76, \"batch_size\": \"1\", \"loss\": 2062.4956889805217}, {\"iter\": 77, \"batch_size\": \"1\", \"loss\": 2096.9643956853974}, {\"iter\": 78, \"batch_size\": \"1\", \"loss\": 2121.662607175779}, {\"iter\": 79, \"batch_size\": \"1\", \"loss\": 2091.4131804166964}, {\"iter\": 80, \"batch_size\": \"1\", \"loss\": 2269.266779900445}, {\"iter\": 81, \"batch_size\": \"1\", \"loss\": 2100.360260819788}, {\"iter\": 82, \"batch_size\": \"1\", \"loss\": 2132.0882628705576}, {\"iter\": 83, \"batch_size\": \"1\", \"loss\": 2130.4128901715767}, {\"iter\": 84, \"batch_size\": \"1\", \"loss\": 2139.997950731002}, {\"iter\": 85, \"batch_size\": \"1\", \"loss\": 2237.1989765402755}, {\"iter\": 86, \"batch_size\": \"1\", \"loss\": 2176.2389874410997}, {\"iter\": 87, \"batch_size\": \"1\", \"loss\": 2963.1239485927968}, {\"iter\": 88, \"batch_size\": \"1\", \"loss\": 2554.595232032584}, {\"iter\": 89, \"batch_size\": \"1\", \"loss\": 2316.4032925065017}, {\"iter\": 90, \"batch_size\": \"1\", \"loss\": 2188.698231056864}, {\"iter\": 91, \"batch_size\": \"1\", \"loss\": 2148.6333589764827}, {\"iter\": 92, \"batch_size\": \"1\", \"loss\": 2149.41419863216}, {\"iter\": 93, \"batch_size\": \"1\", \"loss\": 2117.8323160240493}, {\"iter\": 94, \"batch_size\": \"1\", \"loss\": 5286.63964448778}, {\"iter\": 95, \"batch_size\": \"1\", \"loss\": 5639.484337267768}, {\"iter\": 96, \"batch_size\": \"1\", \"loss\": 2671.1576765492464}, {\"iter\": 97, \"batch_size\": \"1\", \"loss\": 2371.525054532317}, {\"iter\": 98, \"batch_size\": \"1\", \"loss\": 2281.604128820908}, {\"iter\": 99, \"batch_size\": \"1\", \"loss\": 2160.401145150255}, {\"iter\": 100, \"batch_size\": \"1\", \"loss\": 2127.2103057603663}, {\"iter\": 0, \"batch_size\": \"10\", \"loss\": 5793.811507862259}, {\"iter\": 1, \"batch_size\": \"10\", \"loss\": 5531.492067116258}, {\"iter\": 2, \"batch_size\": \"10\", \"loss\": 5217.3737400898735}, {\"iter\": 3, \"batch_size\": \"10\", \"loss\": 4603.570963825092}, {\"iter\": 4, \"batch_size\": \"10\", \"loss\": 4229.555574459421}, {\"iter\": 5, \"batch_size\": \"10\", \"loss\": 3953.6292325296454}, {\"iter\": 6, \"batch_size\": \"10\", \"loss\": 3959.666941502819}, {\"iter\": 7, \"batch_size\": \"10\", \"loss\": 3713.612427443597}, {\"iter\": 8, \"batch_size\": \"10\", \"loss\": 3172.2861609198026}, {\"iter\": 9, \"batch_size\": \"10\", \"loss\": 3034.5649577972554}, {\"iter\": 10, \"batch_size\": \"10\", \"loss\": 2831.6970514607424}, {\"iter\": 11, \"batch_size\": \"10\", \"loss\": 2628.1126175321942}, {\"iter\": 12, \"batch_size\": \"10\", \"loss\": 2504.702827297553}, {\"iter\": 13, \"batch_size\": \"10\", \"loss\": 2435.3492857116044}, {\"iter\": 14, \"batch_size\": \"10\", \"loss\": 2388.010077070792}, {\"iter\": 15, \"batch_size\": \"10\", \"loss\": 2283.8172159756323}, {\"iter\": 16, \"batch_size\": \"10\", \"loss\": 2239.3687537582864}, {\"iter\": 17, \"batch_size\": \"10\", \"loss\": 2238.32187272549}, {\"iter\": 18, \"batch_size\": \"10\", \"loss\": 2131.870205159041}, {\"iter\": 19, \"batch_size\": \"10\", \"loss\": 2126.0638275307765}, {\"iter\": 20, \"batch_size\": \"10\", \"loss\": 2090.450743820381}, {\"iter\": 21, \"batch_size\": \"10\", \"loss\": 2047.8931753410748}, {\"iter\": 22, \"batch_size\": \"10\", \"loss\": 1942.4617586825734}, {\"iter\": 23, \"batch_size\": \"10\", \"loss\": 1961.321164063074}, {\"iter\": 24, \"batch_size\": \"10\", \"loss\": 1903.176878405635}, {\"iter\": 25, \"batch_size\": \"10\", \"loss\": 1927.4387941677942}, {\"iter\": 26, \"batch_size\": \"10\", \"loss\": 1893.5026592129796}, {\"iter\": 27, \"batch_size\": \"10\", \"loss\": 2021.3108073020192}, {\"iter\": 28, \"batch_size\": \"10\", \"loss\": 1980.9787727020023}, {\"iter\": 29, \"batch_size\": \"10\", \"loss\": 1898.2227325162746}, {\"iter\": 30, \"batch_size\": \"10\", \"loss\": 1754.303477034027}, {\"iter\": 31, \"batch_size\": \"10\", \"loss\": 1728.470861777049}, {\"iter\": 32, \"batch_size\": \"10\", \"loss\": 1696.2678825287812}, {\"iter\": 33, \"batch_size\": \"10\", \"loss\": 1698.8318037471347}, {\"iter\": 34, \"batch_size\": \"10\", \"loss\": 1701.1733587484164}, {\"iter\": 35, \"batch_size\": \"10\", \"loss\": 1656.771122832542}, {\"iter\": 36, \"batch_size\": \"10\", \"loss\": 1636.9332599559648}, {\"iter\": 37, \"batch_size\": \"10\", \"loss\": 1626.5410998330633}, {\"iter\": 38, \"batch_size\": \"10\", \"loss\": 1607.0511822821059}, {\"iter\": 39, \"batch_size\": \"10\", \"loss\": 1616.7624499367132}, {\"iter\": 40, \"batch_size\": \"10\", \"loss\": 1624.2170804642765}, {\"iter\": 41, \"batch_size\": \"10\", \"loss\": 1574.9185871460309}, {\"iter\": 42, \"batch_size\": \"10\", \"loss\": 1551.1311245311924}, {\"iter\": 43, \"batch_size\": \"10\", \"loss\": 1535.8673310939591}, {\"iter\": 44, \"batch_size\": \"10\", \"loss\": 1522.5218164342434}, {\"iter\": 45, \"batch_size\": \"10\", \"loss\": 1503.3046839214242}, {\"iter\": 46, \"batch_size\": \"10\", \"loss\": 1495.0380612372282}, {\"iter\": 47, \"batch_size\": \"10\", \"loss\": 1497.3685907424895}, {\"iter\": 48, \"batch_size\": \"10\", \"loss\": 1493.0764976649245}, {\"iter\": 49, \"batch_size\": \"10\", \"loss\": 1495.8865376977662}, {\"iter\": 50, \"batch_size\": \"10\", \"loss\": 1484.866676963168}, {\"iter\": 51, \"batch_size\": \"10\", \"loss\": 1497.7053056315997}, {\"iter\": 52, \"batch_size\": \"10\", \"loss\": 1539.5635094459687}, {\"iter\": 53, \"batch_size\": \"10\", \"loss\": 1513.9317764664333}, {\"iter\": 54, \"batch_size\": \"10\", \"loss\": 1515.9060016992091}, {\"iter\": 55, \"batch_size\": \"10\", \"loss\": 1515.2727690229651}, {\"iter\": 56, \"batch_size\": \"10\", \"loss\": 1531.9242011940976}, {\"iter\": 57, \"batch_size\": \"10\", \"loss\": 1632.140193966229}, {\"iter\": 58, \"batch_size\": \"10\", \"loss\": 1521.0692694945717}, {\"iter\": 59, \"batch_size\": \"10\", \"loss\": 1581.1078825372188}, {\"iter\": 60, \"batch_size\": \"10\", \"loss\": 1620.0655429665821}, {\"iter\": 61, \"batch_size\": \"10\", \"loss\": 1580.0193188899084}, {\"iter\": 62, \"batch_size\": \"10\", \"loss\": 1512.0691603332612}, {\"iter\": 63, \"batch_size\": \"10\", \"loss\": 1490.8659212676246}, {\"iter\": 64, \"batch_size\": \"10\", \"loss\": 1501.1757758045833}, {\"iter\": 65, \"batch_size\": \"10\", \"loss\": 1687.07961797316}, {\"iter\": 66, \"batch_size\": \"10\", \"loss\": 1696.0952667256533}, {\"iter\": 67, \"batch_size\": \"10\", \"loss\": 1528.0316520607719}, {\"iter\": 68, \"batch_size\": \"10\", \"loss\": 1517.5775696144676}, {\"iter\": 69, \"batch_size\": \"10\", \"loss\": 1521.9430030832557}, {\"iter\": 70, \"batch_size\": \"10\", \"loss\": 1536.6484153156869}, {\"iter\": 71, \"batch_size\": \"10\", \"loss\": 1536.8305408551907}, {\"iter\": 72, \"batch_size\": \"10\", \"loss\": 1608.45364248483}, {\"iter\": 73, \"batch_size\": \"10\", \"loss\": 1511.4546909218618}, {\"iter\": 74, \"batch_size\": \"10\", \"loss\": 1509.350037695985}, {\"iter\": 75, \"batch_size\": \"10\", \"loss\": 1514.7978393660176}, {\"iter\": 76, \"batch_size\": \"10\", \"loss\": 1613.2916229976404}, {\"iter\": 77, \"batch_size\": \"10\", \"loss\": 1499.087494212822}, {\"iter\": 78, \"batch_size\": \"10\", \"loss\": 1567.7573893061403}, {\"iter\": 79, \"batch_size\": \"10\", \"loss\": 1502.8125214016477}, {\"iter\": 80, \"batch_size\": \"10\", \"loss\": 1569.9130464645116}, {\"iter\": 81, \"batch_size\": \"10\", \"loss\": 1601.8103603393915}, {\"iter\": 82, \"batch_size\": \"10\", \"loss\": 1495.606249436781}, {\"iter\": 83, \"batch_size\": \"10\", \"loss\": 1874.654041786439}, {\"iter\": 84, \"batch_size\": \"10\", \"loss\": 1591.7372581818643}, {\"iter\": 85, \"batch_size\": \"10\", \"loss\": 1491.2318054327197}, {\"iter\": 86, \"batch_size\": \"10\", \"loss\": 1485.1090486178925}, {\"iter\": 87, \"batch_size\": \"10\", \"loss\": 1486.163401578486}, {\"iter\": 88, \"batch_size\": \"10\", \"loss\": 1504.6973357460174}, {\"iter\": 89, \"batch_size\": \"10\", \"loss\": 1516.4967292794563}, {\"iter\": 90, \"batch_size\": \"10\", \"loss\": 1508.7081854772964}, {\"iter\": 91, \"batch_size\": \"10\", \"loss\": 1510.5974055678141}, {\"iter\": 92, \"batch_size\": \"10\", \"loss\": 1578.5386245736588}, {\"iter\": 93, \"batch_size\": \"10\", \"loss\": 1501.025035748067}, {\"iter\": 94, \"batch_size\": \"10\", \"loss\": 1524.4788617631873}, {\"iter\": 95, \"batch_size\": \"10\", \"loss\": 1530.8192223358428}, {\"iter\": 96, \"batch_size\": \"10\", \"loss\": 1530.7582483235385}, {\"iter\": 97, \"batch_size\": \"10\", \"loss\": 1528.539207461264}, {\"iter\": 98, \"batch_size\": \"10\", \"loss\": 1506.157876546288}, {\"iter\": 99, \"batch_size\": \"10\", \"loss\": 1504.536506418139}, {\"iter\": 100, \"batch_size\": \"10\", \"loss\": 1504.002942761985}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import altair as alt\n",
    "\n",
    "#charting loss by iteration\n",
    "#When the batch size is small, the loss is bouncing around a lot\n",
    "    #this is because the gradient for any one point is kind of random\n",
    "#When the batch size is large, it is more stable\n",
    "    #And lower loss achieved more broadly\n",
    "alt.Chart(df).mark_line().encode(\n",
    "    x='iter',\n",
    "    y='loss',\n",
    "    color=\"batch_size\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking a sneak peak\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "N = 10000 #Number of instances in our dataset\n",
    "dim_ = 10 #Number of dimensions (features)\n",
    "\n",
    "#Instantiating the weight vector as a uniform distribution\n",
    "#low sample is -5, high sample is 2, and the number of items is our dim #\n",
    "#all random numbers\n",
    "w = np.random.uniform(low=-5, high=2, size=dim_)\n",
    "\n",
    "X, y = init_data(N, dim_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X is 10k samples with 10 features\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We would expect the shape of y to be 10k (1 label for each instance)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We would expect the weight vector to be 10 (1 weight for each feature)\n",
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Since z in the dot product of X and w, its shape will be the number\n",
    "#of instances (this is our score vector), one score per instance\n",
    "z = X.dot(w)\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-8.47247171, -7.98054957, -8.84796539])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0:3].dot(w) #since this is 3 instances, it gives us 3 scalars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.8"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#a dot product on a single row of x\n",
    "#the dot product of the weights and one instance gives us a scalar\n",
    "x_0 = [1,0,0,1]\n",
    "w = [.2,.1,.3,-3]\n",
    "dot = 1 * .2- + 0 * .1 + 0 * -3 + 1 * -3\n",
    "dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions: normalization\n",
    "- Complete the L2 norm function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What does the variable `lambda` do in the code above? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What happens if you set `lambda` to a huge number? What happens if you set `lambda` to a small number?  What should you see in terms of accuracy and the norm of the weights? Try systematically varying lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions: Stochastic gradient descent\n",
    "- Print the loss and vary the batch size:\n",
    "    - How do you think that varying eta will vary the amount of noise in the loss?\n",
    "    - How do you think that varying batch size will vary the amount of noise in the loss?\n",
    "    \n",
    "- Test your answers to the previous two questions by making a plot. Your plot should show the loss each iteration, for different batch sizes. You should try batch sizes of 1, 10 and 100. What do you observe in your plot?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
