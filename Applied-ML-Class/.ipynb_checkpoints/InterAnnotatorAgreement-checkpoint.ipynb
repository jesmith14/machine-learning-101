{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "all_results = []\n",
    "\n",
    "with open(\"merged.csv\", \"r\") as inf:\n",
    "    results = csv.reader(inf)\n",
    "    next(results)\n",
    "    for r in results:\n",
    "        annotation, review, annotator = r\n",
    "        all_results.append({\"annotator\": annotator, \"annotation\": annotation, \"review\": review})\n",
    "\n",
    "annotators = set()\n",
    "\n",
    "from collections import defaultdict\n",
    "review2judgements = defaultdict(list)\n",
    "\n",
    "for result in all_results:\n",
    "    review2judgements[result[\"review\"]].append({\"annotator\": result[\"annotator\"], \"annotation\": result[\"annotation\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many annotators are there? \n",
    "\n",
    "[Your answer here]\n",
    "\n",
    "### How many data points are there? \n",
    "\n",
    "[Your answer here]\n",
    "\n",
    "### Which review seems to be least controverial? Most controversial?\n",
    "\n",
    "[Your answer here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_agreement(results):\n",
    "    '''\n",
    "    Compute the pairwise agreement between raters for the input results\n",
    "    \n",
    "    To compute pairwise agreement compare judgements from all pairs of annotators for a given item\n",
    "    Return the fraction of pairs of annotators who agree\n",
    "    '''\n",
    "    total_judgements = {}\n",
    "    for result in results:\n",
    "        for other_result in results:\n",
    "            if result[\"annotator\"] != other_result[\"annotator\"]:\n",
    "                pair = [result[\"annotator\"], other_result[\"annotator\"]]\n",
    "                pair.sort()\n",
    "                pair = \"-\".join(pair)\n",
    "                total_judgements[pair] = (result[\"annotation\"], other_result[\"annotation\"])\n",
    "    out = total_judgements.values()\n",
    "    agrees = 0\n",
    "    for pair in out:\n",
    "        judgement1, judgement2 = pair \n",
    "        if judgement1 == judgement2:\n",
    "            agrees += 1\n",
    "    return agrees/len(out)\n",
    "\n",
    "review4 = {\"1\": 1, \"2\": 0, \"3\": 1}\n",
    "\n",
    "# do 1 and 2 agree ==> No\n",
    "\n",
    "# do 1 and 3 agree ==> Yes\n",
    "\n",
    "# do 2 and 3 agree ==> No\n",
    "\n",
    "# agreement rate = 1/3 = #agreements / #pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per-item analysis\n",
    "\n",
    "- Which review has the highest and lowest pairwise agreement rate? Does this make sense?\n",
    "\n",
    "[Type your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random agreement rate\n",
    "\n",
    "If two reviewers answered randomly (meaning just picked random annotations) how often would they agree just by chance?\n",
    "\n",
    "[Type your answer here, and explain your reasoning]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\bar{P}_e$ = .5 = \"change agreement rate\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fleiss Kappa\n",
    "\n",
    "[Fleiss kappa](https://en.wikipedia.org/wiki/Fleiss%27_kappa) measures the exent to which pairs of reviewers agree, as compared to how much they would agree by chance. \n",
    "\n",
    "- $\\bar{P}_e$ is the rate at which reviewers agree by chance \n",
    "- $\\bar{P}$ is the pairwise agreement rate across all items the dataset\n",
    "    - note: the Wikipedia article uses a slightly different definition of $\\bar{P}$, because it assumes all reviewers review all items, which is not true in our case\n",
    "\n",
    "\n",
    "$\\kappa = \\frac{\\bar{P} - \\bar{P}_e}{1-\\bar{P}_e}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What is the highest possible value of Fleiss Kappa? What is the lowest?\n",
    "\n",
    "[Type your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What is the highest possible value of Fleiss Kappa? What is the lowest?\n",
    "\n",
    "[Type your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What does the denominator mean? If $\\bar{P}_e$ is high, then is the denominator high or low?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If $\\bar{P}$ is high and $\\bar{P_e}$ is high, do you think the task is well-defined?\n",
    "\n",
    "[Type your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If $\\bar{P}$ is high and $\\bar{P_e}$ is high, do you think the task is well-defined\n",
    "\n",
    "[Type your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If $\\bar{P}$ is high and $\\bar{P_e}$ is low, do you think the task is well-defined?\n",
    "\n",
    "[Type your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What do you think the Fleiss Kappa will be for the Yelp data set? Do you think it will be higher or lower than for the emotions dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Fleiss Kappa for the dataset\n",
    "\n",
    "def kappa(Pe, Pbar):\n",
    "    return 0\n",
    "\n",
    "# Fill out this function "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
