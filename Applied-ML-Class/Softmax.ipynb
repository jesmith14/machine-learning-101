{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The softmax function\n",
    "\n",
    "\n",
    "The softmax function can be written as follows, where $i$ indexes an instance in a dataset and there are $K$ output classes. \n",
    "\n",
    "$p(y_i = k | x_i) = \\frac{\\text{exp}(\\text{score}_{i, k})}{\\sum_{k'}^K \\text{exp}(\\text{score}_{i,{k^\\prime}})} $\n",
    "\n",
    "In this notebook we will implement the softmax function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Thinking through the dimensions of our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.46733923,  0.53773009,  0.095832  ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "K = 3   # let K be the number of classes\n",
    "J = 2   # let J be the number of features\n",
    "\n",
    "W = np.random.rand(K, J)  \n",
    "\n",
    "#one instance is one row\n",
    "#one instance has two features in this scenario (J=2)\n",
    "x_i = np.asarray([-1,1])\n",
    "\n",
    "scores = W.dot(x_i)  \n",
    "# scores should be a 1D vector of raw scores, one for each class\n",
    "# scores aren't probabilites because they need to be normalized\n",
    "# without normalization, they could be bigger than 1 or less than 0\n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Plotting the exp function\n",
    "\n",
    "- Plot the exp function from -3 to 3. \n",
    "\n",
    "- What happens to the domain as the range gets very big or small?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-4204eda4a2124de9a51c510bf85413fb\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-4204eda4a2124de9a51c510bf85413fb\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-4204eda4a2124de9a51c510bf85413fb\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-d97ec4b0e1c6a4c61d7903743354b12e\"}, \"mark\": \"line\", \"encoding\": {\"x\": {\"type\": \"quantitative\", \"field\": \"x\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"e^x\"}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-d97ec4b0e1c6a4c61d7903743354b12e\": [{\"x\": -3.0, \"e^x\": 0.049787068367863944}, {\"x\": -2.877551020408163, \"e^x\": 0.056272404194356196}, {\"x\": -2.7551020408163267, \"e^x\": 0.0636025292836268}, {\"x\": -2.63265306122449, \"e^x\": 0.07188748711185018}, {\"x\": -2.510204081632653, \"e^x\": 0.08125165557821261}, {\"x\": -2.387755102040816, \"e^x\": 0.09183561422767024}, {\"x\": -2.2653061224489797, \"e^x\": 0.10379825470085506}, {\"x\": -2.142857142857143, \"e^x\": 0.11731916609425078}, {\"x\": -2.020408163265306, \"e^x\": 0.13260133104084862}, {\"x\": -1.8979591836734695, \"e^x\": 0.14987417298618502}, {\"x\": -1.7755102040816326, \"e^x\": 0.1693970004069815}, {\"x\": -1.653061224489796, \"e^x\": 0.19146289967870542}, {\"x\": -1.5306122448979593, \"e^x\": 0.21640313503371336}, {\"x\": -1.4081632653061225, \"e^x\": 0.24459212166433136}, {\"x\": -1.2857142857142858, \"e^x\": 0.2764530466295643}, {\"x\": -1.163265306122449, \"e^x\": 0.3124642219492766}, {\"x\": -1.0408163265306123, \"e^x\": 0.3531662652616454}, {\"x\": -0.9183673469387754, \"e^x\": 0.39917021584348367}, {\"x\": -0.795918367346939, \"e^x\": 0.451166707835154}, {\"x\": -0.6734693877551021, \"e^x\": 0.5099363383830838}, {\"x\": -0.5510204081632653, \"e^x\": 0.5763613863515786}, {\"x\": -0.4285714285714288, \"e^x\": 0.6514390575310555}, {\"x\": -0.30612244897959195, \"e^x\": 0.7362964551863366}, {\"x\": -0.18367346938775508, \"e^x\": 0.8322075006903012}, {\"x\": -0.06122448979591866, \"e^x\": 0.9406120582638511}, {\"x\": 0.06122448979591821, \"e^x\": 1.0631375509322778}, {\"x\": 0.18367346938775508, \"e^x\": 1.2016233922074937}, {\"x\": 0.30612244897959195, \"e^x\": 1.358148600276674}, {\"x\": 0.4285714285714284, \"e^x\": 1.5350630092552096}, {\"x\": 0.5510204081632653, \"e^x\": 1.7350225460627982}, {\"x\": 0.6734693877551021, \"e^x\": 1.961029102516639}, {\"x\": 0.7959183673469385, \"e^x\": 2.216475601221393}, {\"x\": 0.9183673469387754, \"e^x\": 2.5051969318074176}, {\"x\": 1.0408163265306118, \"e^x\": 2.831527522197352}, {\"x\": 1.1632653061224492, \"e^x\": 3.2003664091895097}, {\"x\": 1.2857142857142856, \"e^x\": 3.617250785229936}, {\"x\": 1.408163265306122, \"e^x\": 4.088439125493831}, {\"x\": 1.5306122448979593, \"e^x\": 4.621005143221287}, {\"x\": 1.6530612244897958, \"e^x\": 5.222943983811503}, {\"x\": 1.7755102040816322, \"e^x\": 5.903292251913959}, {\"x\": 1.8979591836734695, \"e^x\": 6.672263673422753}, {\"x\": 2.020408163265306, \"e^x\": 7.54140242899933}, {\"x\": 2.1428571428571423, \"e^x\": 8.523756461042597}, {\"x\": 2.2653061224489797, \"e^x\": 9.634073355876593}, {\"x\": 2.387755102040816, \"e^x\": 10.889021741836384}, {\"x\": 2.5102040816326525, \"e^x\": 12.307441526990209}, {\"x\": 2.63265306122449, \"e^x\": 13.91062673319063}, {\"x\": 2.7551020408163263, \"e^x\": 15.72264517249992}, {\"x\": 2.8775510204081627, \"e^x\": 17.77069976513095}, {\"x\": 3.0, \"e^x\": 20.085536923187668}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import altair as alt\n",
    "\n",
    "x = np.linspace(-3, 3, 50)\n",
    "y = np.exp(x) #vectorized code to exponentiate all instances of x\n",
    "\n",
    "df = pd.DataFrame({'x':x, 'e^x':y})\n",
    "\n",
    "#charting out the exponential function\n",
    "alt.Chart(df).mark_line().encode(\n",
    "x=\"x\",\n",
    "y=\"e^x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Naive softmax\n",
    "$p(y_i = k | x_i) = \\frac{\\text{exp}(\\text{score}_{i, k})}{\\sum_{k'}^K \\text{exp}(\\text{score}_{i,{k^\\prime}})} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5, 0.5]\n",
      "[0.5, 0.5]\n",
      "[0.5, 0.5]\n",
      "[2.1315982402125493e-22, 1.0]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def naive_sofmax(scores):\n",
    "    '''\n",
    "    Write a \"naive\" version of softmax that does not use vectorized operations\n",
    "    This will be *way* slower than the numpy version, but that is OK\n",
    "    We are just building intuition before jumping to the vectorized version\n",
    "    When you don't know how to code something in ML, it can be good to start with a \n",
    "    simple version using loops before skipping ahead to the vectorized version\n",
    "    '''\n",
    "    #computing the numerator:\n",
    "    output = []\n",
    "    for item in scores:\n",
    "        output.append(item)\n",
    "#     n_0 = math.exp(scores[0])\n",
    "#     n_1 = math.exp(scores[1])\n",
    "#     n_2 = math.exp(scores[2])\n",
    "    #computing the denominator\n",
    "#     denominator = n_0 + n_1 + n_2\n",
    "    \n",
    "    numerator = []\n",
    "    for s in scores:\n",
    "        numerator.append(math.exp(s))\n",
    "    \n",
    "    #computing the denominator\n",
    "    denominator = sum(numerator)\n",
    "    \n",
    "    for i in range(len(scores)):\n",
    "        output[i] = numerator[i]/denominator\n",
    "        \n",
    "    return output\n",
    "    \n",
    "print(naive_sofmax([5,5]))\n",
    "#this is the same thing because softmax returns probabilities,\n",
    "#so it doesn't matter how big the score is, it's the proportion of the score\n",
    "print(naive_sofmax([50,50]))\n",
    "#this gives us basically zero probability of being the first class\n",
    "print(naive_sofmax([0.1,50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Vectorized code (aside)\n",
    "\n",
    "Vectorized operations are much faster. A vectorized operation uses linear algebra packages like numpy to do computation very quickly using highly optimized code. Some implementations also implement vector operations in parallel.\n",
    "\n",
    "Let's experiment with vectorized versions of a simple function summing a list of numbers, before moving on to a vectorized softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.23606797749979, 2.23606797749979)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math \n",
    "\n",
    "#this uses a 'type hint' for the arguments\n",
    "#it's more for the developers than for Python\n",
    "#so it isn't actually enforced, but you can use it..\n",
    "def L2norm_no_vector(x: list):\n",
    "    '''\n",
    "    Your code here should not use vector operations\n",
    "    \n",
    "    input: a vector\n",
    "    output, L2 norm of x, \\sqrt{\\Sigma x_i^2}\n",
    "    '''\n",
    "    #this solution only works with 2 items,\n",
    "    #so instead we want a for loop\n",
    "#     x0_2 = x[0] ^ 2\n",
    "#     x1_2 = x[1] ^ 2\n",
    "#     sum_ = x0_2 + x1_2    \n",
    "    \n",
    "    sum_ = 0\n",
    "    for x_j in x:\n",
    "        sum_ += x_j ** 2 #'w'\n",
    "        \n",
    "    return math.sqrt(sum_) #return a float\n",
    "        \n",
    "def L2norm_with_vectors(x: np.ndarray):\n",
    "    '''\n",
    "    Your code here should use vector operations\n",
    "\n",
    "    input: a vector\n",
    "    output, L2 norm of x, \\sqrt{\\Sigma x_i^2}\n",
    "    '''\n",
    "    return np.sqrt(np.sum(np.square(x)))\n",
    "\n",
    "\n",
    "a = [1,2]\n",
    "b = np.asarray([1,2])\n",
    "L2norm_no_vector(a), L2norm_no_vector(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before going on, let's confirm that both implementations return the same outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.385164807134504, 5.385164807134504)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L2norm_no_vector([2,3,4]),  L2norm_with_vectors(np.asarray([2,3,4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test how long it takes our code to run as the list grows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-f1f73438d1f94b618f7328b08112f7a9\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-f1f73438d1f94b618f7328b08112f7a9\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-f1f73438d1f94b618f7328b08112f7a9\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-e572853ee152f99245254984af833c76\"}, \"mark\": \"line\", \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"f\"}, \"x\": {\"type\": \"quantitative\", \"field\": \"N\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"time\"}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-e572853ee152f99245254984af833c76\": [{\"N\": 1, \"f\": \"L2norm_no_vector\", \"time\": 1.2733917683362961e-05}, {\"N\": 1001, \"f\": \"L2norm_no_vector\", \"time\": 0.0003943522000918165}, {\"N\": 2001, \"f\": \"L2norm_no_vector\", \"time\": 0.0007953813613858074}, {\"N\": 3001, \"f\": \"L2norm_no_vector\", \"time\": 0.0011065252003027127}, {\"N\": 4001, \"f\": \"L2norm_no_vector\", \"time\": 0.001336418041610159}, {\"N\": 5001, \"f\": \"L2norm_no_vector\", \"time\": 0.001579023320809938}, {\"N\": 6001, \"f\": \"L2norm_no_vector\", \"time\": 0.0016341621609171852}, {\"N\": 7001, \"f\": \"L2norm_no_vector\", \"time\": 0.0018993015604792164}, {\"N\": 8001, \"f\": \"L2norm_no_vector\", \"time\": 0.0021591537207132205}, {\"N\": 9001, \"f\": \"L2norm_no_vector\", \"time\": 0.002479842318571173}, {\"N\": 1, \"f\": \"L2norm_with_vectors\", \"time\": 4.3700398528017104e-05}, {\"N\": 1001, \"f\": \"L2norm_with_vectors\", \"time\": 1.3824880588799716e-05}, {\"N\": 2001, \"f\": \"L2norm_with_vectors\", \"time\": 4.6254719491116704e-05}, {\"N\": 3001, \"f\": \"L2norm_with_vectors\", \"time\": 2.5733159272931516e-05}, {\"N\": 4001, \"f\": \"L2norm_with_vectors\", \"time\": 3.2390919514000416e-05}, {\"N\": 5001, \"f\": \"L2norm_with_vectors\", \"time\": 6.264799972996115e-05}, {\"N\": 6001, \"f\": \"L2norm_with_vectors\", \"time\": 4.409420071169734e-05}, {\"N\": 7001, \"f\": \"L2norm_with_vectors\", \"time\": 5.788276088424027e-05}, {\"N\": 8001, \"f\": \"L2norm_with_vectors\", \"time\": 5.5854999809525905e-05}, {\"N\": 9001, \"f\": \"L2norm_with_vectors\", \"time\": 6.552028004080058e-05}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "results = []\n",
    "\n",
    "for n in range(1, 10000, 1000):\n",
    "    runs = []\n",
    "    for i in range(25): # take an average of 25 runs\n",
    "        start = timer()\n",
    "        L2norm_no_vector(np.random.rand(n))\n",
    "        end = timer()\n",
    "        runs.append(end - start)\n",
    "    mean = np.mean(runs)\n",
    "    results.append({\"N\": n, \"f\": 'L2norm_no_vector', \"time\": mean})\n",
    "    \n",
    "for n in range(1, 10000, 1000):\n",
    "    runs = []\n",
    "    for i in range(25): # take an average of 25 runs\n",
    "        start = timer()\n",
    "        L2norm_with_vectors(np.random.rand(n))\n",
    "        end = timer()\n",
    "        runs.append(end - start)\n",
    "    mean = np.mean(runs)\n",
    "    results.append({\"N\": n, \"f\": 'L2norm_with_vectors', \"time\": mean})\n",
    "\n",
    "results = pd.DataFrame(results)\n",
    "\n",
    "#x axis: N is the size of the vector (how big it is)\n",
    "#y axis: How long it takes to find the norm of the vector\n",
    "#We should expect the plot to show that the larger the vector,\n",
    "#the more time it will take of the no_vector solution\n",
    "#and it will take drastically less time for the vectorized solution\n",
    "alt.Chart(results).mark_line().encode(\n",
    "    x=\"N\",\n",
    "    y=\"time\",\n",
    "    color=\"f\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorized softmax\n",
    "\n",
    "Now we are ready to take our first crack at a vectorized softmax function. Remember the equation for the softmax is as follows:\n",
    "\n",
    "$p(y_i = k | x_i) = \\frac{\\text{exp}(\\text{score}_{i, k})}{\\sum_{k'}^K \\text{exp}(\\text{score}_{i,{k^\\prime}})} $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.99876605e-01 1.23394576e-04]\n",
      "[1.00000000e+00 1.01122149e-43]\n",
      "[nan  0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-27-1a144c07da02>:9: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(scores)/np.sum(np.exp(scores))\n",
      "<ipython-input-27-1a144c07da02>:9: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(scores)/np.sum(np.exp(scores))\n"
     ]
    }
   ],
   "source": [
    "def softmax(scores):\n",
    "    '''\n",
    "    input: \n",
    "        a scores function, of $K$ real values between -\\inf and \\inf\n",
    "        \n",
    "    output:\n",
    "        a vector of probabilities that sums to 1\n",
    "    '''\n",
    "    return np.exp(scores)/np.sum(np.exp(scores))\n",
    "\n",
    "#this returns really low probability on the right side\n",
    "print(softmax([10, 1]))\n",
    "#the left class is close to 1.0 probability\n",
    "print(softmax([100, 1]))\n",
    "#now we get an error!\n",
    "print(softmax([1000, 1]))\n",
    "\n",
    "#Error explanation:\n",
    "#When we exponentiate numbers greater than 200, the exponentiation\n",
    "#gets SUPER big -- and it reaches a point where it's too big for our\n",
    "#machine. We get an 'overflow encountered' because there are too many\n",
    "#bits that we are using for the size of memory we have allocated\n",
    "#SOLUTION: whenever you get numerical instability in ML, one of the common\n",
    "#solutions is to use logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax questions\n",
    "\n",
    "1. What happens when the scores are the same?\n",
    "2. What happens when one score is bigger than the others\n",
    "3. Why do you think this function might be called a \"soft\" max?\n",
    "4. What happens if you pass in the scores `[10000, 1]`\n",
    "4. What happens if you pass in the scores `[0, 1]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerically stable softmax \n",
    "\n",
    "\n",
    "$p(y_i = k | x_i) = \\frac{\\text{exp}(\\text{score}_{i, k})}{\\sum_{k'}^K \\text{exp}(\\text{score}_{i,{k^\\prime}})}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is some max score function $c$ in the vector of score functions. We can always divide the numerator and denominator of a fraction by $c$. It will affect the value of the numerator and denominator but not the ratio. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p(y_i = k | x_i) = \\frac{\\text{exp}(\\text{score}_{i, k}) / e^c}{\\sum_{k'}^K \\text{exp}(\\text{score}_{i,{k^\\prime}})/ e^c}$\n",
    "\n",
    "$p(y_i = k | x_i) = \\frac{\\text{exp}(\\text{score}_{i, k}) exp(-c)}{\\sum_{k'}^K \\text{exp}(\\text{score}_{i,{k^\\prime}}) exp(-c)}$\n",
    "\n",
    "$p(y_i = k | x_i) = \\frac{\\text{exp}(\\text{score}_{i, k} - c) }{\\sum_{k'}^K \\text{exp}(\\text{score}_{i,{k^\\prime} } -c ) }$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3/9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(3/2)/(9/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p(y_i = k | x_i) = \\frac{\\text{exp}(\\text{score}_{i, k} - c)}{\\sum_{k'}^K \\text{exp}(\\text{score}_{i,{k^\\prime}}  - c)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.26894142, 0.73105858]), array([0.26894142, 0.73105858]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax_c(scores, c):\n",
    "    '''\n",
    "    input: \n",
    "        a scores function, of $K$ real values between -\\inf and \\inf\n",
    "        \n",
    "    output:\n",
    "        a vector of probabilities that sums to 1\n",
    "    '''\n",
    "    return np.exp(scores - c)/np.sum(np.exp(scores - c))\n",
    "\n",
    "softmax(np.asarray([1,2])),  softmax_c(np.asarray([1,2]), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's take the log of the softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p(y_i = k | x_i) = \\frac{\\text{exp}(\\text{score}_{i, k} - c)}{\\sum_{k'}^K \\text{exp}(\\text{score}_{i,{k^\\prime}}  - c)}$\n",
    "\n",
    "$ log p(y_i = k | x_i) = log (\\text{score}_{i, k}) - c  - log {\\sum_{k'}^K \\text{exp}(\\text{score}_{i,{k^\\prime}}  - c)}$\n",
    "\n",
    "We can use any $c$ we want so let's set it to the largest item in our scores vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BIGNUM 7.22597376812575e+86\n",
      "logbignum 200.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7.225973768125749e+86"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#doing the log of exponent and doing exponent again\n",
    "#gets us the same number we started with\n",
    "bignum = np.exp(100) * np.exp(100)\n",
    "print('BIGNUM', bignum)\n",
    "logbignum = np.log(bignum)\n",
    "print('logbignum', logbignum)\n",
    "np.exp(logbignum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.5, 0.5]), array([-0.69314718, -0.69314718]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we do this operation last, it works because we are doing\n",
    "#the operations on the numerator and denominator simultaneously\n",
    "def log_softmax(scores):\n",
    "    '''\n",
    "    input: \n",
    "        a scores function, of $K$ real values between -\\inf and \\inf\n",
    "        \n",
    "    output:\n",
    "        a vector of probabilities that sums to 1\n",
    "    '''\n",
    "    #choose c to be the biggest of the score function\n",
    "    #you can choose any c you want though cause its just a constant\n",
    "    c = np.max(scores)\n",
    "    sum_exp = np.sum(np.exp(scores - c))\n",
    "    return scores - c - np.log(sum_exp)\n",
    "\n",
    "#to get the softmax back, we exponentiate the log softmax function\n",
    "np.exp(log_softmax([1, 1])), log_softmax([1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
